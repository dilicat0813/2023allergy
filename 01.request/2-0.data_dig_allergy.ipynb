{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rnum                0\n",
       "prdlstReportNo      0\n",
       "productGb           0\n",
       "prdlstNm            0\n",
       "rawmtrl             0\n",
       "allergy            19\n",
       "prdkind             1\n",
       "prdkindstate        2\n",
       "manufacture         0\n",
       "imgurl1             0\n",
       "imgurl2             0\n",
       "nutrient          300\n",
       "seller            194\n",
       "barcode           321\n",
       "capacity          367\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "#1. 바코드 nan --> 알수없음\n",
    "#2. capacity nan --> 알수없음\n",
    "#3, seller --> 알수없음\n",
    "#4. 영양 --> 알수 없음\n",
    "#5. prdkindstate --> 알수 없음\n",
    "#6. prdkind --> 알수없음\n",
    "#allergy 19 null--> 알수없음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy = df[['rnum','rawmtrl','allergy']]\n",
    "df_allergy = pd.DataFrame(df_allergy)\n",
    "df_allergy = df_allergy.fillna('알수없음')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 맞춤법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#치명적 오탈자 교체\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('복훙아', '복숭아')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('함유식품', '')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('식품', '')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace(' 함유', '')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('함유', '')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('함류', '')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('포함', '')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('소고기', '쇠고기')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('괘지', '돼지')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('돼고기', '돼지고기')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('오지엉', '오징어')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('날류','난류')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('유우','우유')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('유유','우유')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('우류','우유')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('탕콩','땅콩')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('대듀','대두')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('토마투','토마토')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('토마도','토마토')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('토미토','토마토')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('이황산류', '아황산류')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('이황산','아황산')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('이산화항','이산화황')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('아산화류', '아황산류')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('아산화','이산화')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('아산화항','이산화황')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('쇠구기','쇠고기')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('·',' ')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('토마','토마토')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('토마토토','토마토')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('게란','계란')\n",
    "df_allergy.loc[14737, 'allergy'] = '밀, 대두, 계란, 우유, 게, 오징어, 새우, 쇠고기, 조개류'\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('닭괴','닭고기')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('쇠소기','쇠고기')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('호도','호두')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('조래규','조개류')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('굴','조개류')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('홍합','조개류')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('바지락','조개류')\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('백합','조개류') \n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('전복','조개류') \n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace('등','') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #조개류 통일\n",
    "pattern = r'조개류\\([^\\)]+\\)'\n",
    "df_allergy['allergy'] = [re.sub(pattern, '조개류', s) for s in df_allergy['allergy']]\n",
    "df_allergy['allergy'] = [re.sub(pattern, '조개류', s) for s in df_allergy['allergy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = '[^ㄱ-ㅎㅏ-ㅣ가-힣()[\\],]+' # 한글, 괄호, 대괄호, 쉼표, 마침표를 제외한 모든 문자\n",
    "df_allergy['rawmtrl_normalize'] = df_allergy['rawmtrl'].apply(lambda x: re.sub(pattern, ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawmtrl 맞춤법 검사 --> 오타가 있음(소백분 같은 경우)\n",
    "# 우선 정규화를 먼저 진행하고, 그다음에 진행\n",
    "\n",
    "from hanspell import spell_checker\n",
    "import json\n",
    "df_allergy['rawmtrl_checked'] = \"\"\n",
    "datas = df_allergy['rawmtrl_normalize'] \n",
    "for i, n in enumerate(datas):\n",
    "    try: \n",
    "        input = n.replace(\"&\",\"\").replace(\"*\",\"\").replace(\"{\",\"\").replace(\"}\",\"\").replace(\"/\",\"\") #&가 있으면 xml 문자열에서 에러를 일으킴\n",
    "        result = spell_checker.check(input)\n",
    "        result_dict = result.as_dict()\n",
    "        df_allergy.at[i, 'rawmtrl_checked'] = result_dict['checked']\n",
    "    except:\n",
    "        df_allergy.at[i, 'rawmtrl_checked'] = n\n",
    "        pass\n",
    "\n",
    "df_allergy.to_csv('data/0515.csv')\n",
    "#88분 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy.to_csv('data/0515.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy = pd.read_csv('data/0515.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy['rawmtrl_checked'] = df_allergy['rawmtrl_checked'].str.replace('탈지 대도','탈지대두')\n",
    "df_allergy['rawmtrl_checked'] = df_allergy['rawmtrl_checked'].str.replace('대도','대두')\n",
    "df_allergy['rawmtrl_checked'] = df_allergy['rawmtrl_checked'].str.replace('뉴 크림','유크림')\n",
    "df_allergy['rawmtrl_checked'] = df_allergy['rawmtrl_checked'].str.replace('자란 액','전란액')\n",
    "df_allergy['rawmtrl_checked'] = df_allergy['rawmtrl_checked'].str.replace('난 황','난황')\n",
    "df_allergy['rawmtrl_checked'] = df_allergy['rawmtrl_checked'].str.replace('난 각','난각')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "df_allergy = pd.read_csv('data/0508.csv')\n",
    "df = df_allergy[['rnum','rawmtrl','allergy']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # allergy 맞춤법 검사 오히려 오타가 늘어..ㅋㅋㅋ\n",
    "# df_allergy['allergy_checked'] = \"\"\n",
    "# datas = df_allergy['allergy'] \n",
    "# for i, n in enumerate(datas):\n",
    "#     input = n.replace(\"&\",\"\")\n",
    "#     result = spell_checker.check(input)\n",
    "#     result_dict = result.as_dict()\n",
    "#     df_allergy.at[i, 'allergy_checked'] = result_dict['checked']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. null, 특이사례, 불필요 대상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#알러지2칼럼만 사용\n",
    "#자연어 처리를 통해 rawmtrl 중 알러지 유발 가능 대상을 mtr_allergy에 추가할 수 있음 (1번은 새우, 멸치가 알러지 유발 대상인데, 알러지 정보는 없다고 나와 있음) --> 새로운 컬럼으로 배당, 제공정보 중 제품에서 분석된 것으로 설명\n",
    "#allergy의 없음, 알수 없음, null에 해당하는 대상들을 자연어 처리로 해결\n",
    "#분류된 알러지 정보들을 숫자로 변환, 취합\n",
    "#단, 페닐알라닌과 같이 19개의 번호에 포함되지 않는 경우, 기타(페닐알라닌,.... ) 과 같이 표기할 수 있도록 함\n",
    "#\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 알러지 딕셔너리, 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allergy = ['알류(가금류)', '우유', '메밀', '땅콩', '대두', '밀', '고등어', '게', '새우', '돼지고기', '복숭아', '토마토', '아황산류', '호두', '닭고기', '쇠고기', '오징어', '조개류', '잣']\n",
    "# allergy_dict = {1: '알류(가금류)', 2: '우유', 3: '메밀', 4: '땅콩', 5: '대두', 6: '밀', 7: '고등어', 8: '게', 9: '새우', 10: '돼지고기', 11: '복숭아', 12: '토마토', 13: '아황산류', 14: '호두', 15: '닭고기', 16: '쇠고기', 17: '오징어', 18: '조개류', 19: '잣'}\n",
    "# allergy = pd.DataFrame(allergy)\n",
    "# allergy.reset_index(inplace=True)\n",
    "# allergy['index']= allergy['index']+1\n",
    "# allergy.to_csv('data/allergy_table.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 알러지에 문자열 넣은 경우 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ',' 로 구분된 열들을 분리하여 리스트로 변환\n",
    "# allergy_list = df_allergy['allergy'].str.split(',')\n",
    "\n",
    "# # 리스트를 바탕으로 새로운 열들을 생성\n",
    "# new_columns = ['allergy_{}'.format(i+1) for i in range(allergy_list.str.len().max())]\n",
    "# new_df = df_allergy.assign(**{col: allergy_list.str[i] for i, col in enumerate(new_columns)}) #대충 분리가능한지 검토 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['allergy_20'].notnull()] #아래 있는 것들은 아마도 성분 그냥 때려박은 것으로 보임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv('data/allergy_search_0504.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df['allergy_20'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['allergy_5'] == \" 계\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/allergy_search_0504.csv')\n",
    "# df['allergy_1'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF 성분 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy['rawmtrl_checked'][9033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 알러지 성분 추출"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## allergy 컬럼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "df_allergy['allergy'] = df_allergy['allergy'].str.replace(\" \",\"\")\n",
    "datas = df_allergy['allergy']\n",
    "\n",
    "# allergy_dict 미리 정의\n",
    "allergy_dict = {1: ['알류', '계란', '달걀', '난류','메추리알','메추리','추리알','난각','난백','난황','전란'], \n",
    "                2: ['우유','탈지분유','분유','유청','카제인','유크림','유청'], \n",
    "                3: '메밀', \n",
    "                4: ['땅콩', '탕콩'],\n",
    "                5: ['대두','두부','콩','적두','유부'], \n",
    "                6: ['밀','밀가루','소맥','소맥분','밀제품'], \n",
    "                7: '고등어', \n",
    "                8: '게', \n",
    "                9: ['새우'], \n",
    "                10: ['돼지고기','돈','돼지','돼고기','돈창', '돈혈', '돈지방', '돈육'],\n",
    "                11: ['복숭아','황도','백도','천도'], \n",
    "                12: ['토마토','토마'],\n",
    "                13: ['아황산', '아황산류', '아황산나트륨','이산화','이산화황'],\n",
    "                14: ['호두'], \n",
    "                15: ['닭고기', '계육','닭','치킨'], \n",
    "                16: ['쇠고기', '우육', '소고기', '소뼈'], \n",
    "                17: ['오징어'],\n",
    "                18: ['조개류', '조개'], \n",
    "                19: '잣',\n",
    "                #20: ['아몬드','헤이즐넛','참깨','피칸'],\n",
    "                #21: ['페닐알라닌']\n",
    "                }              \n",
    "\n",
    "allergy_nums = []\n",
    "for data in datas:\n",
    "    data = str(data)\n",
    "    okt = Okt()\n",
    "    words = okt.pos(data)\n",
    "    allergy_columns = []\n",
    "\n",
    "    for word, tag in words:\n",
    "        if tag == 'Noun':\n",
    "            for allergy_key, allergy_value in allergy_dict.items():\n",
    "                if isinstance(allergy_value, list):\n",
    "                    if word in allergy_value:\n",
    "                        allergy_columns.append(allergy_key)\n",
    "                        break\n",
    "                else:\n",
    "                    if word == allergy_value:\n",
    "                        allergy_columns.append(allergy_key)\n",
    "                        break\n",
    "        \n",
    "    allergy_num = ','.join(str(x) for x in allergy_columns) # 문자열로 변환\n",
    "    allergy_nums.append(allergy_num)\n",
    "\n",
    "df_allergy['allergy_num'] = allergy_nums # DataFrame에 열 추가"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rawmtrl 컬럼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "datas = df_allergy['rawmtrl']\n",
    "\n",
    "# allergy_dict 미리 정의\n",
    "allergy_dict = {1: ['알류', '계란', '달걀', '난류','메추리알','메추리','추리알','난각','날류','난백','난황','난각','게란','전란','흰자','노른자'], \n",
    "                2: ['우유','유우', '탈지분유','유유','분유','유청','카제인','우류','가공버터','유크림','크림','유청','치즈','원유'], \n",
    "                3: '메밀', \n",
    "                4: ['땅콩', '탕콩'],\n",
    "                5: ['대두','두부','콩','유부','된장','대두유','대두단백'], \n",
    "                6: ['밀','밀가루','소맥','소맥분', '밀제품'], \n",
    "                7: ['고등어'], \n",
    "                8: ['게'], \n",
    "                9: ['새우'], \n",
    "                10: ['돼지','돈','돼지고기','괘지','괘지고기','돼고기','돈창', '돈혈', '돈지방','괘','젤라틴'],\n",
    "                11: ['복숭아','황도','백도','천도'], \n",
    "                12: ['토마토', '토마투','토마도','토마','토미토'],\n",
    "                13: ['아황산', '아황산나트륨', '이황산', '이산화', '아산화', '이산화황','아산화황','이산화항','이황산류'],\n",
    "                14: ['호두','호도'], \n",
    "                15: ['닭', '닭고기', '계육','닭괴','치킨','닭다리'], \n",
    "                16: ['쇠고기', '우육', '소고기', '소뼈','쇠구기', '한우', '우정육', '소정육','소뼈','소잡뼈','우골'], \n",
    "                17: ['오징어', '오지엉','오지','지엉'],\n",
    "                18: ['조개류','굴','홍합','조개','백합','바지락','전복'], \n",
    "                19: '잣',\n",
    "                #20: ['아몬드','헤이즐넛','참깨','피칸'],\n",
    "                #21: ['페닐알라닌']\n",
    "                }\n",
    "                \n",
    "allergy_nums = []\n",
    "for data in datas:\n",
    "    data = str(data)\n",
    "    okt = Okt()\n",
    "    words = okt.pos(data)\n",
    "    allergy_columns = []\n",
    "\n",
    "    for word, tag in words:\n",
    "        if tag == 'Noun':\n",
    "            for allergy_key, allergy_value in allergy_dict.items():\n",
    "                if isinstance(allergy_value, list):\n",
    "                    if word in allergy_value:\n",
    "                        allergy_columns.append(allergy_key)\n",
    "                        break\n",
    "                else:\n",
    "                    if word == allergy_value:\n",
    "                        allergy_columns.append(allergy_key)\n",
    "                        break\n",
    "        \n",
    "    allergy_num = ','.join(str(x) for x in allergy_columns) # 문자열로 변환\n",
    "    allergy_nums.append(allergy_num)\n",
    "\n",
    "df_allergy['rawmtrl_num'] = allergy_nums # DataFrame에 열 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rawmtrl_checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "df_allergy['rawmtrl_checked'] = df_allergy['rawmtrl_checked'].str.replace(\" \",\"\")\n",
    "datas = df_allergy['rawmtrl_checked']\n",
    "\n",
    "# allergy_dict 미리 정의\n",
    "allergy_dict = {1: ['알류', '계란', '달걀', '난류','메추리알','메추리','추리알','난각','날류','난백','난황','난각','게란','전란','흰자','노른자'], \n",
    "                2: ['우유','유우', '탈지분유','유유','분유','유청','카제인','우류','가공버터','유크림','크림','유청','치즈','원유'], \n",
    "                3: '메밀', \n",
    "                4: ['땅콩', '탕콩'],\n",
    "                5: ['대두','두부','콩','유부','된장','대두유','대두단백'], \n",
    "                6: ['밀','밀가루','소맥','소맥분', '밀제품'], \n",
    "                7: ['고등어'], \n",
    "                8: ['게'], \n",
    "                9: ['새우'], \n",
    "                10: ['돼지','돈','돼지고기','괘지','괘지고기','돼고기','돈창', '돈혈', '돈지방','괘','젤라틴'],\n",
    "                11: ['복숭아','황도','백도','천도'], \n",
    "                12: ['토마토', '토마투','토마도','토마','토미토'],\n",
    "                13: ['아황산', '아황산나트륨', '이황산', '이산화', '아산화', '이산화황','아산화황','이산화항','이황산류'],\n",
    "                14: ['호두','호도'], \n",
    "                15: ['닭', '닭고기', '계육','닭괴','치킨','닭다리'], \n",
    "                16: ['쇠고기', '우육', '소고기', '소뼈','쇠구기', '한우', '우정육', '소정육','소뼈','소잡뼈','우골'], \n",
    "                17: ['오징어', '오지엉','오지','지엉'],\n",
    "                18: ['조개류','굴','홍합','조개','백합','바지락','전복'], \n",
    "                19: '잣',\n",
    "                #20: ['아몬드','헤이즐넛','참깨','피칸'],\n",
    "                #21: ['페닐알라닌']\n",
    "                }\n",
    "                \n",
    "allergy_nums = []\n",
    "for data in datas:\n",
    "    data = str(data)\n",
    "    okt = Okt()\n",
    "    words = okt.pos(data)\n",
    "    allergy_columns = []\n",
    "\n",
    "    for word, tag in words:\n",
    "        if tag == 'Noun':\n",
    "            for allergy_key, allergy_value in allergy_dict.items():\n",
    "                if isinstance(allergy_value, list):\n",
    "                    if word in allergy_value:\n",
    "                        allergy_columns.append(allergy_key)\n",
    "                        break\n",
    "                else:\n",
    "                    if word == allergy_value:\n",
    "                        allergy_columns.append(allergy_key)\n",
    "                        break\n",
    "        \n",
    "    allergy_num = ','.join(str(x) for x in allergy_columns) # 문자열로 변환\n",
    "    allergy_nums.append(allergy_num)\n",
    "\n",
    "df_allergy['rawmtrl_num2'] = allergy_nums # DataFrame에 열 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from konlpy.tag import Okt\n",
    "\n",
    "# # 한글 정규화 함수\n",
    "# def normalize_text(text):\n",
    "#     # 중복 공백 제거\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "#     # 한글 정규화\n",
    "#     text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "#     return text.strip()\n",
    "\n",
    "# # Mecab 토크나이저 초기화\n",
    "# tokenizer = Okt()\n",
    "\n",
    "# # 문장 정규화 후 토큰화\n",
    "# df_allergy['tokens1'] = df_allergy['rawmtrl'].apply(normalize_text).apply(tokenizer.morphs)\n",
    "# df_allergy['tokens2'] = df_allergy['rawmtrl_checked'].apply(normalize_text).apply(tokenizer.morphs)\n",
    "\n",
    "# # 결과 확인\n",
    "# print(df_allergy.head())\n",
    "\n",
    "import re\n",
    "from soynlp.normalizer import normalize\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "#한글 정규화 함수\n",
    "def normalize_text(text):\n",
    "    # 중복 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # 한글 정규화\n",
    "    text = normalize(text)\n",
    "    return text.strip()\n",
    "\n",
    "#soynlp 토크나이저 초기화\n",
    "tokenizer = MaxScoreTokenizer()\n",
    "\n",
    "#문장 정규화 후 토큰화\n",
    "df_allergy['tokens1'] = df_allergy['rawmtrl'].apply(normalize_text).apply(tokenizer.tokenize)\n",
    "#df_allergy['tokens2'] = df_allergy['rawmtrl_checked'].apply(normalize_text).apply(tokenizer.tokenize)\n",
    "\n",
    "#결과 확인\n",
    "print(df_allergy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "datas = df_allergy['tokens1']\n",
    "\n",
    "# allergy_dict 미리 정의\n",
    "allergy_dict = {1: ['알류', '계란', '달걀', '난류','메추리알','메추리','추리알','난각','날류','난백','난황','난각','게란','전란','흰자','노른자'], \n",
    "                2: ['우유','유우', '탈지분유','유유','분유','유청','카제인','우류','가공버터','유크림','크림','유청','치즈','원유'], \n",
    "                3: '메밀', \n",
    "                4: ['땅콩', '탕콩'],\n",
    "                5: ['대두','두부','콩','유부','된장','대두유','대두단백'], \n",
    "                6: ['밀','밀가루','소맥','소맥분', '밀제품'], \n",
    "                7: ['고등어'], \n",
    "                8: ['게'], \n",
    "                9: ['새우'], \n",
    "                10: ['돼지','돈','돼지고기','괘지','괘지고기','돼고기','돈창', '돈혈', '돈지방','괘','젤라틴'],\n",
    "                11: ['복숭아','황도','백도','천도'], \n",
    "                12: ['토마토', '토마투','토마도','토마','토미토'],\n",
    "                13: ['아황산', '아황산나트륨', '이황산', '이산화', '아산화', '이산화황','아산화황','이산화항','이황산류'],\n",
    "                14: ['호두','호도'], \n",
    "                15: ['닭', '닭고기', '계육','닭괴','치킨','닭다리'], \n",
    "                16: ['쇠고기', '우육', '소고기', '소뼈','쇠구기', '한우', '우정육', '소정육','소뼈','소잡뼈','우골'], \n",
    "                17: ['오징어', '오지엉','오지','지엉'],\n",
    "                18: ['조개류','굴','홍합','조개','백합','바지락','전복'], \n",
    "                19: '잣',\n",
    "                #20: ['아몬드','헤이즐넛','참깨','피칸'],\n",
    "                #21: ['페닐알라닌']\n",
    "                }\n",
    "                \n",
    "allergy_nums = []\n",
    "for data in datas:\n",
    "    data = str(data)\n",
    "    okt = Okt()\n",
    "    words = okt.pos(data)\n",
    "    allergy_columns = []\n",
    "\n",
    "    for word, tag in words:\n",
    "        if tag == 'Noun':\n",
    "            for allergy_key, allergy_value in allergy_dict.items():\n",
    "                if isinstance(allergy_value, list):\n",
    "                    if word in allergy_value:\n",
    "                        allergy_columns.append(allergy_key)\n",
    "                        break\n",
    "                else:\n",
    "                    if word == allergy_value:\n",
    "                        allergy_columns.append(allergy_key)\n",
    "                        break\n",
    "        \n",
    "    allergy_num = ','.join(str(x) for x in allergy_columns) # 문자열로 변환\n",
    "    allergy_nums.append(allergy_num)\n",
    "\n",
    "df_allergy['rawmtrl_num3'] = allergy_nums # DataFrame에 열 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "datas = df_allergy['tokens2']\n",
    "\n",
    "# allergy_dict 미리 정의\n",
    "allergy_dict = {1: ['알류', '계란', '달걀', '난류','메추리알','메추리','추리알','난각','날류','난백','난황','난각','게란','전란','흰자','노른자'], \n",
    "                2: ['우유','유우', '탈지분유','유유','분유','유청','카제인','우류','가공버터','유크림','크림','유청','치즈','원유'], \n",
    "                3: '메밀', \n",
    "                4: ['땅콩', '탕콩'],\n",
    "                5: ['대두','두부','콩','유부','된장','대두유','대두단백'], \n",
    "                6: ['밀','밀가루','소맥','소맥분', '밀제품'], \n",
    "                7: ['고등어'], \n",
    "                8: ['게'], \n",
    "                9: ['새우'], \n",
    "                10: ['돼지','돈','돼지고기','괘지','괘지고기','돼고기','돈창', '돈혈', '돈지방','괘','젤라틴'],\n",
    "                11: ['복숭아','황도','백도','천도'], \n",
    "                12: ['토마토', '토마투','토마도','토마','토미토'],\n",
    "                13: ['아황산', '아황산나트륨', '이황산', '이산화', '아산화', '이산화황','아산화황','이산화항','이황산류'],\n",
    "                14: ['호두','호도'], \n",
    "                15: ['닭', '닭고기', '계육','닭괴','치킨','닭다리'], \n",
    "                16: ['쇠고기', '우육', '소고기', '소뼈','쇠구기', '한우', '우정육', '소정육','소뼈','소잡뼈','우골'], \n",
    "                17: ['오징어', '오지엉','오지','지엉'],\n",
    "                18: ['조개류','굴','홍합','조개','백합','바지락','전복'], \n",
    "                19: '잣',\n",
    "                #20: ['아몬드','헤이즐넛','참깨','피칸'],\n",
    "                #21: ['페닐알라닌']\n",
    "                }\n",
    "                \n",
    "allergy_nums = []\n",
    "for data in datas:\n",
    "    data = str(data)\n",
    "    okt = Okt()\n",
    "    words = okt.pos(data)\n",
    "    allergy_columns = []\n",
    "\n",
    "    for word, tag in words:\n",
    "        if tag == 'Noun':\n",
    "            for allergy_key, allergy_value in allergy_dict.items():\n",
    "                if isinstance(allergy_value, list):\n",
    "                    if word in allergy_value:\n",
    "                        allergy_columns.append(allergy_key)\n",
    "                        break\n",
    "                else:\n",
    "                    if word == allergy_value:\n",
    "                        allergy_columns.append(allergy_key)\n",
    "                        break\n",
    "        \n",
    "    allergy_num = ','.join(str(x) for x in allergy_columns) # 문자열로 변환\n",
    "    allergy_nums.append(allergy_num)\n",
    "\n",
    "df_allergy['rawmtrl_num4'] = allergy_nums # DataFrame에 열 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy['num_sum'] = df_allergy['allergy_num']+','+df_allergy['rawmtrl_num']+','+df_allergy['rawmtrl_num2']+','+df_allergy['rawmtrl_num3']+','+df_allergy['rawmtrl_num4']\n",
    "df_allergy['num_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(str_nums):\n",
    "    nums = set(str_nums.split(','))\n",
    "    nums.discard('')\n",
    "    return ','.join(sorted(nums))\n",
    "\n",
    "#df_allergy['num_sum'] = df_allergy['num_sum'].apply(remove_duplicates)\n",
    "df_allergy['num_sum'] = df_allergy['num_sum'].apply(remove_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allergy_dict = {1: '난류(가금류)', 2: '우유', 3: '메밀', 4: '땅콩', 5: '콩', 6: '밀', 7: '고등어', 8: '게', 9: '새우', 10: '돼지고기', 11: '복숭아', 12: '토마토', 13: '아황산류', 14: '호두', 15: '닭고기', 16: '쇠고기', 17: '오징어', 18: '조개류', 19: '잣'}  #, 20:'견과류', 21:'페닐알라닌'} 삭제\n",
    "\n",
    "def convert_to_allergy(num_str):\n",
    "    num_list = num_str.split(',')\n",
    "    allergy_list = []\n",
    "    for num in num_list:\n",
    "        if num.strip() == '':\n",
    "            continue\n",
    "        allergy_list.append(allergy_dict[int(num)])\n",
    "    return ','.join(allergy_list)\n",
    "\n",
    "#df_allergy['new_allergy'] = df_allergy['num_sum'].apply(convert_to_allergy)\n",
    "df_allergy['new_allergy'] = df_allergy['num_sum'].apply(convert_to_allergy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_allergy.iterrows():\n",
    "    if row['new_allergy'] == \"\":\n",
    "        df_allergy.loc[index, 'sentence'] = row['rawmtrl'] + '의 알레르기 유발성분은 없다.'\n",
    "    else:\n",
    "        df_allergy.loc[index, 'sentence'] = row['rawmtrl'] + '의 알레르기 유발성분은 ' + row['new_allergy'] + '이다.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # 정규표현식 패턴: 한글 외의 문자 1개 이상을 모두 제거\n",
    "# pattern = '[^ㄱ-ㅎㅏ-ㅣ가-힣]+'\n",
    "\n",
    "# # 'sentence' 컬럼의 모든 값을 정규화하여 'normalized' 컬럼에 저장\n",
    "# df_allergy['normalized'] = df_allergy['sentence'].apply(lambda x: re.sub(pattern, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 한글 정규화 함수\n",
    "def normalize_text(text):\n",
    "    # 중복 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # 한글 정규화\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Mecab 토크나이저 초기화\n",
    "tokenizer = Okt()\n",
    "\n",
    "# 문장 정규화 후 토큰화\n",
    "df_allergy['tokens'] = df_allergy['sentence'].apply(normalize_text).apply(tokenizer.morphs)\n",
    "\n",
    "# 결과 확인\n",
    "print(df_allergy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터와 검증 데이터로 분리\n",
    "train_data, test_data = train_test_split(df_allergy['tokens'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "model = Word2Vec(sentences=train_data, vector_size=150, window=5, min_count=5, workers=12, sg=1)\n",
    "\n",
    "# 모델 저장\n",
    "model.save('allergy_word2vec.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 검증 데이터로 평가\n",
    "# accuracy = model.wv.evaluate_word_pairs('test_word_pairs.txt', delimiter='\\t')\n",
    "# print('Accuracy:', accuracy)\n",
    "\n",
    "# # 검증 데이터로 일부 추가 학습\n",
    "# model.train(test_data, total_examples=len(test_data), epochs=10)\n",
    "\n",
    "# # 검증 데이터로 평가\n",
    "# accuracy = model.wv.evaluate_word_pairs('test_word_pairs.txt', delimiter='\\t')\n",
    "# print('Accuracy after fine-tuning:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 모델의 단어 벡터를 추출\n",
    "word_vectors = model.wv\n",
    "\n",
    "# 가장 유사한 단어 찾기\n",
    "similar_words = {search_term: [item[0] for item in word_vectors.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['소고기', '돼지고기','닭고기','대두','콩', '새우','게','아황산']}\n",
    "\n",
    "# 찾은 단어들에 대한 단어 벡터 가져오기\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = model.wv[words]\n",
    "\n",
    "# t-SNE를 사용하여 단어 벡터 시각화\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=5)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=T[:, 0], y=T[:, 1], hue=[label.split(':')[0] for label in labels])\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(T[:, 0].min()-50, T[:, 0].max()+50)\n",
    "    plt.ylim(T[:, 1].min()-50, T[:, 1].max()+50)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
